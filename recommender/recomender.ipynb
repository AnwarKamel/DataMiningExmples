{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. MovieLens 100K Dataset: EDA via Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_100k_BASE_URL = 'ml-100k/'\n",
    "\n",
    "def movielens_url(fname):\n",
    "    return ML_100k_BASE_URL+fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1. Read Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of the columns - see README, u.genre\n",
    "genre_cols = ['genre', 'genre_id']\n",
    "\n",
    "genres = pd.read_csv(movielens_url('u.genre'), sep='|', names=genre_cols)\n",
    "\n",
    "genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2. Read Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "\n",
    "# Read users from u.user\n",
    "# user id | age | gender | occupation | zip code\n",
    "users = pd.read_csv(movielens_url('u.user'), sep='|', names=user_cols)\n",
    "\n",
    "users.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3. Read Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_cols = ['movie_id','movie_title','release_date','video_release_date','IMDb_URL'] + [g for g in genres.sort_values(by='genre_id')['genre']]\n",
    "\n",
    "# Read movies from u.item\n",
    "movies = pd.read_csv(movielens_url('u.item'), sep='|', names=movie_cols, encoding='latin-1')\n",
    "\n",
    "movies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting... find the row with a null release_date!\n",
    "movies.loc[movies.release_date.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign movie_id (to check/clean up ratings later)\n",
    "bad_row_id = 266\n",
    "\n",
    "# drop the row (in place!)\n",
    "movies.drop(movies.loc[pd.isnull(movies.release_date)].index, inplace=True)\n",
    "\n",
    "# Confirm it's gone\n",
    "movies.loc[movies.release_date.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output a table including only the movies with null IMDB urls\n",
    "movies.loc[movies.IMDb_URL.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output all the fields of the first row\n",
    "movies.loc[1357]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output all the fields of the second row\n",
    "movies.loc[1358]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a mistake. The title should've been `Boys`(http://www.imdb.com/title/tt0115742/) instead of `Boys from Venice`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movies with unknown genre?\n",
    "movies.loc[movies.unknown==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a field of the movies dataset that has no useful values - what is it?\n",
    "bad_field = 'video_release_date'\n",
    "movies.drop(bad_field, 1, inplace=True)\n",
    "\n",
    "# It also turns out that the IMDB url's don't work :(\n",
    "movies.drop('IMDb_URL', 1, inplace=True)\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.4. Read Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "\n",
    "# Read ratings from u.data\n",
    "ratings = pd.read_csv(movielens_url('u.data'), sep='\\t', names=rating_cols)\n",
    "\n",
    "ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to drop ratings of our bad movie above\n",
    "ratings.drop(ratings.loc[ratings.movie_id==1373].index, 0, inplace=True)\n",
    "# Confirm they are gone\n",
    "ratings.loc[ratings.movie_id==1373]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.5. Merge Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = pd.merge(pd.merge(movies, ratings), users)\n",
    "lens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.6. Ask Interesting Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What percentage of the full utility matrix is represented?\n",
    "\n",
    "sparsity = (ratings.shape[0] / (users.shape[0] * movies.shape[0])) * 100\n",
    "print('Sparsity: {:.2f}%'.format(sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by movie_id, get stats on number (i.e. size) of ratings per movie\n",
    "ratings.groupby('movie_id')['rating'].count().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the movie counts as a histogram\n",
    "ratings.groupby('movie_id')['rating'].count().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by user_id, get stats on number (i.e. size) of ratings per user\n",
    "ratings.groupby('user_id')['rating'].count().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the user counts as a histogram\n",
    "ratings.groupby('user_id')['rating'].count().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very few movies have been rated and most of the ratings are by a small subset of the users. The above two graphs exhibit the long tail phenomenon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which movie has the most ratings?\n",
    "movies.loc[movies.movie_id==ratings.groupby('movie_id')['rating'].count().idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are all ratings equally likely? output a bar chart of rating frequency overall\n",
    "ratings.rating.hist(grid=False,range=(1,6), align='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of the movies that have 300 or more ratings, what are the top-ten ranked movies?\n",
    "top_movies = ratings.groupby('movie_id').agg({'user_id':np.size, 'rating':np.mean}).reset_index()\n",
    "top_movies = top_movies.rename(columns={'user_id': 'size', 'rating': 'rating mean'})\n",
    "top_movies = top_movies[top_movies['size'] >= 300]\n",
    "top_movies = pd.merge(movies[['movie_id', 'movie_title']], top_movies).sort_values('rating mean', ascending=False)\n",
    "top_movies = top_movies.set_index('movie_id')\n",
    "top_movies.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Extract Utility Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_id_to_index(id):\n",
    "    return id-1\n",
    "\n",
    "def user_index_to_id(idx):\n",
    "    return idx+1\n",
    "\n",
    "# Accounts for the bad movie\n",
    "def movie_id_to_index(id):\n",
    "    return id-1 if id<bad_row_id else id-2\n",
    "\n",
    "# Accounts for the bad movie\n",
    "def movie_index_to_id(idx):\n",
    "    return idx+1 if idx<(bad_row_id-1) else idx+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = ratings.user_id.unique().shape[0]\n",
    "num_movies = ratings.movie_id.unique().shape[0]\n",
    "\n",
    "utility = np.zeros((num_users, num_movies))\n",
    "\n",
    "for idx,rating in ratings.iterrows():\n",
    "    utility[user_id_to_index(rating.user_id), movie_id_to_index(rating.movie_id)] = rating.rating\n",
    "\n",
    "# should match above!\n",
    "sparsity = float(len(utility.nonzero()[0]))\n",
    "sparsity /= (utility.shape[0] * utility.shape[1])\n",
    "sparsity *= 100\n",
    "print('Sparsity: {:.2f}%'.format(sparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Evaluation via Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4, 5]\n",
    "b = [1, 1, 4, 2, 10]\n",
    "print(\"{:.4f}\".format(mse(a, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_utility(u1, u2):\n",
    "    return mse(u1[u1.nonzero()].flatten(), u2[u2.nonzero()].flatten())\n",
    "\n",
    "u1 = np.array([[2, 0, 0, 4, 4], [5, 5, 5, 3, 3], [2, 4, 2, 1, 2]])\n",
    "u2 = np.array([[3, 0, 0, 4, 4], [5, 5, 5, 3, 3], [2, 4, 2, 1, 7]])\n",
    "\n",
    "print(\"{:.4f}\".format(mse_utility(u1, u2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Similarity via Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing only using primitive Python loops/operations\n",
    "# sim = (A . B) / ( ||A|| ||B|| )\n",
    "def cosine_sim(v1, v2):\n",
    "    \n",
    "    def l2(v):\n",
    "        return sum(n**2 for n in v)**(0.5)\n",
    "    \n",
    "    numerator = sum(n1*n2 for n1, n2 in zip(v1, v2))\n",
    "    denominator = l2(v1) * l2(v2)\n",
    "    return numerator / denominator\n",
    "    \n",
    "v1 = [5, 0, 3, 0, 2, 0, 0, 2, 0, 0]\n",
    "v2 = [3, 0, 2, 0, 1, 1, 0, 1, 0, 1]\n",
    "\n",
    "print(\"Cosine Similarity: {:.6f}, expected={:.6f}\".format(cosine_sim(v1, v2), cosine_similarity(np.array(v1).reshape(1, -1), np.array(v2).reshape(1, -1))[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_matrix(u, eps=1.0e-9):\n",
    "    step1 = u.dot(u.T) + eps\n",
    "    step2 = np.array([np.sqrt(np.diagonal(step1))])\n",
    "    return (step1 / step2 / step2.T)\n",
    "\n",
    "print(sim_matrix(u1))\n",
    "print(cosine_sim(u1[0], u1[1]))\n",
    "print(cosine_sim(u1[0], u1[2]))\n",
    "print(cosine_sim(u1[1], u1[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_allpairs(u):\n",
    "    n = u.shape[0]\n",
    "    sim = np.eye(n)\n",
    "    for x in itertools.combinations(range(n), 2):\n",
    "        s = cosine_sim(u[x[0]], u[x[1]])\n",
    "        sim[x[0], x[1]] = s\n",
    "        sim[x[1], x[0]] = s\n",
    "    \n",
    "    return sim\n",
    "\n",
    "print(\"MSE for U1: {:.4f}\".format(mse_utility(sim_matrix(u1), sim_allpairs(u1))))\n",
    "print(\"MSE for U2: {:.4f}\".format(mse_utility(sim_matrix(u2), sim_allpairs(u2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 3 sim_matrix(utility[:50,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 3 sim_allpairs(utility[:50,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. How does `sim_matrix` work?**\n",
    "\n",
    "`step1 = u.dot(u.T) + eps`: Computes a symmetric matrix $S$ where $S_{ij} = u_i \\cdot u_j$. `eps` is needed to avoid `DivisionByZero` error.\n",
    "\n",
    "`step2 = np.array([np.sqrt(np.diagonal(step1))])`: `step2` is an array $A$ where $A_i = ||u_i||_2$\n",
    "\n",
    "`return (step1 / step2 / step2.T)`: Every dot product term in `step1` is divided by the $l_2$ norms of the constituent vectors. Hence element $S_{ij}$ of the final matrix is the cosine distance between vectors $u_i$ and $u_j$.\n",
    "\n",
    "\n",
    "**2. Why is it `sim_matrix` faster than pairwise calls to `cosine_sim`?**\n",
    "\n",
    "Vectorized operations are faster and hence calls to `sim_matrix` is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_users(u):\n",
    "    return sim_matrix(u)\n",
    "\n",
    "# TODO: use sim_matrix to return similarity between items (i.e. movies)\n",
    "def sim_items(u):\n",
    "    return sim_matrix(u.T)\n",
    "\n",
    "print(sim_items(u1))\n",
    "print(cosine_sim(u1[:, 0], u1[:, 1]))\n",
    "print(cosine_sim(u1[:, 0], u1[:, 2]))\n",
    "print(cosine_sim(u1[:, 0], u1[:, 3]))\n",
    "print(cosine_sim(u1[:, 0], u1[:, 4]))\n",
    "print(cosine_sim(u1[:, 1], u1[:, 2]))\n",
    "print(cosine_sim(u1[:, 1], u1[:, 3]))\n",
    "print(cosine_sim(u1[:, 1], u1[:, 4]))\n",
    "print(cosine_sim(u1[:, 2], u1[:, 3]))\n",
    "print(cosine_sim(u1[:, 2], u1[:, 4]))\n",
    "print(cosine_sim(u1[:, 3], u1[:, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. K-Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a slice from a similarity matrix (arr),\n",
    "# index of \"self\" (self_idx),\n",
    "# and k\n",
    "# return dictionary of k most similar indexes as {idx:sim}\n",
    "def top_k(arr, self_idx, k):\n",
    "    ind = arr.argsort()[-k-1:-1]\n",
    "    return {i:arr[i] for i in sorted(ind)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_u1 = sim_items(u1)\n",
    "\n",
    "# Example via u1...\n",
    "print(sim_u1)\n",
    "print()\n",
    "\n",
    "# Top-2 w.r.t. row 0 (should be {1: 0.897, 2:0.937})\n",
    "print(sim_u1[0])\n",
    "print(top_k(sim_u1[0], 0, 2))\n",
    "print()\n",
    "\n",
    "# Top-2 w.r.t. row 1 (should be {0: 0.897, 2:0.957})\n",
    "print(sim_u1[1])\n",
    "print(top_k(sim_u1[1], 1, 2))\n",
    "print()\n",
    "\n",
    "# Top-3 w.r.t. col 1 (should be {0: 0.897, 2:0.957, 4:0.667})\n",
    "print(sim_u1[:, 1])\n",
    "print(top_k(sim_u1[:, 1], 1, 3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F. Recommend via Similar Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#       given utility matrix (m_utility),\n",
    "#       similarity of users (m_sim_users),\n",
    "#       user index of interest (user_idx),\n",
    "#       item index of interest (item_idx),\n",
    "#       neighborhood size (k)\n",
    "# \n",
    "#       output the average of the similarity-weighted ratings\n",
    "#       of top-k similar users that have\n",
    "#       rated the item\n",
    "def rec_via_users(m_utility, m_sim_users, user_idx, item_idx, k):\n",
    "    sim_users = top_k(m_sim_users[user_idx], user_idx, k)\n",
    "    item_ratings = m_utility.T[item_idx]\n",
    "    numer = 0\n",
    "    denom = 0\n",
    "    for i in sim_users:\n",
    "        if item_ratings[i] != 0:\n",
    "            numer += item_ratings[i] * sim_users[i] \n",
    "            denom += sim_users[i]\n",
    "    if denom != 0:\n",
    "        return numer / denom\n",
    "    return 0\n",
    "    \n",
    "print(u1)\n",
    "print()\n",
    "print(sim_users(u1))\n",
    "print()\n",
    "\n",
    "# Expected: 5\n",
    "print(rec_via_users(u1, sim_users(u1), 0, 1, 1))\n",
    "\n",
    "# Expected: 4.54 ~ (.588 * 5) + (.495 * 4) / (.588 + .495)\n",
    "print(rec_via_users(u1, sim_users(u1), 0, 1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F.1. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(12345)\n",
    "\n",
    "def recs_via_users(m_utility, m_sim_users, k, test_n):\n",
    "    test = random.sample(range(m_sim_users.shape[0]), test_n)\n",
    "    true = []\n",
    "    pred = []\n",
    "    for user_idx in test:\n",
    "        for item_idx in range(m_utility.shape[1]):\n",
    "            if m_utility[user_idx][item_idx] != 0:\n",
    "                true.append(m_utility[user_idx][item_idx])\n",
    "                \n",
    "                p = round(rec_via_users(m_utility, m_sim_users, user_idx, item_idx, k))\n",
    "                if p != 0:    \n",
    "                    pred.append(p)\n",
    "                else:\n",
    "                    pred.append(1.0e-9)\n",
    "                        \n",
    "\n",
    "    return mse_utility(np.array([true], dtype=np.float64), np.array([pred], dtype=np.float64))\n",
    "    \n",
    "similarity_users = sim_users(utility)\n",
    "\n",
    "ks = []\n",
    "mses = []\n",
    "for i in range(50):\n",
    "    ks.append(i+1)\n",
    "    mses.append(recs_via_users(utility, similarity_users, i+1, 100))\n",
    "    print(\"{}/50\".format(i+1), mses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 3 recs_via_users(utility, similarity_users, 2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 3 recs_via_users(utility, similarity_users, 20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ks, mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`recs_via_users` picks a random subset of 100 users from the utility matrix and computes the MSE between the true and predicted ratings for all items, parameterized by the neighborhood size `k`. \n",
    "\n",
    "As `k` increases, the MSE betweena actual and predicted ratings reduces.\n",
    "\n",
    "Using the elbow approach, a neighborhood size of 7 seems to be appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G. Recommend via SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(12345)\n",
    "\n",
    "#       given a utility matrix and\n",
    "#       number of singular values to use\n",
    "# \n",
    "#       return a projected matrix\n",
    "#       based upon a limited number of dimensions\n",
    "#       U_pred = U S V^T\n",
    "def svd_projection(m_utility, num_dims):\n",
    "    U, s, Vt = svds(A=m_utility, k=num_dims)\n",
    "    dim = (len(s), len(s))\n",
    "    S = np.zeros(dim, dtype=np.float32)\n",
    "    for i in range(0, len(s)):\n",
    "        S[i,i] = s[i]\n",
    "    return U @ S @ Vt\n",
    "\n",
    "def recs_via_svd(m_utility, num_dims, test_n):\n",
    "    test = random.sample(range(m_utility.shape[0]), test_n)    \n",
    "    svd_utility = svd_projection(utility, num_dims)\n",
    "    \n",
    "    true = []\n",
    "    pred = []\n",
    "    for user_idx in test:\n",
    "        for item_idx in range(m_utility.shape[1]):\n",
    "            if m_utility[user_idx][item_idx] != 0:\n",
    "                true.append(m_utility[user_idx][item_idx])\n",
    "                p = round(svd_utility[user_idx][item_idx])\n",
    "                \n",
    "                if p != 0:\n",
    "                    pred.append(p)\n",
    "                else:\n",
    "                    pred.append(1.0e-9)\n",
    "\n",
    "    return mse_utility(np.array([true], dtype=np.float64), np.array([pred], dtype=np.float64))\n",
    "    \n",
    "    \n",
    "ds = []\n",
    "mses = []\n",
    "for i in range(100):\n",
    "    ds.append(i+1)\n",
    "    mses.append(recs_via_svd(utility, i+1, 100))\n",
    "    print(\"{}/100\".format(i+1), mses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 3 recs_via_svd(utility, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 3 recs_via_svd(utility, 20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 3 recs_via_svd(utility, 40, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 3 recs_via_svd(utility, 80, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds, mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SVD is faster than the neighborhood based approach. This is because the neighborhood-based approach has a runtime of $O(n^2)$\n",
    "2. The quality of the neighborhood based approach is better. This is reflected in the MSE values displayed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H. Recommend via Similar Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_via_items(m_utility, m_sim_items, user_idx, item_idx, k):\n",
    "    sim_items = top_k(m_sim_items[item_idx], item_idx, k)\n",
    "    item_ratings = m_utility[user_idx]\n",
    "    numer = 0\n",
    "    denom = 0\n",
    "    for i in sim_items:\n",
    "        if item_ratings[i] != 0:\n",
    "            numer += item_ratings[i] * sim_items[i] \n",
    "            denom += sim_items[i]\n",
    "    if denom != 0:\n",
    "        return numer / denom\n",
    "    return 0\n",
    "    \n",
    "print(u1)\n",
    "print()\n",
    "print(sim_items(u1))\n",
    "print()\n",
    "\n",
    "# Expected: 0\n",
    "print(rec_via_items(u1, sim_items(u1), 0, 1, 1))\n",
    "\n",
    "# Expected: 4.54 ~ (.588 * 5) + (.495 * 4) / (.588 + .495)\n",
    "print(rec_via_items(u1, sim_items(u1), 0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(12345)\n",
    "\n",
    "def recs_via_items(m_utility, m_sim_items, k, test_n):\n",
    "    test = random.sample(range(m_sim_items.shape[0]), test_n)\n",
    "    true = []\n",
    "    pred = []\n",
    "    for item_idx in test:\n",
    "        for user_idx in range(m_utility.shape[0]):\n",
    "            if m_utility[user_idx][item_idx] != 0:\n",
    "                true.append(m_utility[user_idx][item_idx])\n",
    "                \n",
    "                p = round(rec_via_items(m_utility, m_sim_items, user_idx, item_idx, k))\n",
    "                if p != 0:    \n",
    "                    pred.append(p)\n",
    "                else:\n",
    "                    pred.append(1.0e-9)\n",
    "                        \n",
    "\n",
    "    return mse_utility(np.array([true], dtype=np.float64), np.array([pred], dtype=np.float64))\n",
    "    \n",
    "similarity_items = sim_items(utility)\n",
    "\n",
    "ks = []\n",
    "mses = []\n",
    "for i in range(50):\n",
    "    ks.append(i+1)\n",
    "    mses.append(recs_via_items(utility, similarity_items, i+1, 100))\n",
    "    print(\"{}/50\".format(i+1), mses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 3 recs_via_items(utility, similarity_users, 2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 3 recs_via_items(utility, similarity_users, 20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ks, mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
